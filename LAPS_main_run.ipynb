{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAPS script for the main run\n",
    "Authors: Gabi Mukai, Johanna Wren, Taylor Ely, and Don Kobayashi\n",
    "\n",
    "Date: May 21, 2025\n",
    "\n",
    "## Description\n",
    " \n",
    "Script to run the main run for the LAPS project. This code is based on code developed during the sensitivity analysis and from Gabi's bumphead parrotfish Wake script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "import math \n",
    "from pathlib import Path\n",
    "\n",
    "from parcels import FieldSet,NestedField, ParticleSet, JITParticle, ScipyParticle, AdvectionRK4, DiffusionUniformKh, Variable, Field,GeographicPolar,Geographic\n",
    "from datetime import timedelta as timedelta\n",
    "import datetime\n",
    "from parcels.tools.converters import TimeConverter\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "from copy import copy\n",
    "from os.path import isfile\n",
    "import pytz\n",
    "from os import path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete Error\n",
    "def DeleteErrorParticle(particle, fieldset, time):\n",
    "    if particle.state == StatusCode.ErrorOutOfBounds:\n",
    "        particle.delete()\n",
    "\n",
    "# Delete Kernel\n",
    "def DeleteParticle(particle, fieldset, time):\n",
    "    print('deleted particle')\n",
    "    particle.delete()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717984000\n",
      "730944000\n",
      "start loading in files\n"
     ]
    }
   ],
   "source": [
    "# Define start and end dates\n",
    "startDate = '1992-10-02'\n",
    "endDate = '1993-02-28'\n",
    "\n",
    "# Wake Hycom Files are labeled by their time since epoch\n",
    "# getting the desired start/end dates in time since epoch\n",
    "desired_startdate = datetime.datetime(1992, 10, 2, tzinfo=pytz.utc)  # Year, Month, Day\n",
    "desired_enddate = datetime.datetime(1993, 3, 1, tzinfo=pytz.utc) \n",
    "\n",
    "startseconds_since_epoch = desired_startdate.timestamp()\n",
    "endseconds_since_epoch = desired_enddate.timestamp()\n",
    "startseconds_since_epoch = int(startseconds_since_epoch)\n",
    "endseconds_since_epoch = int(endseconds_since_epoch)\n",
    "\n",
    "print(startseconds_since_epoch)\n",
    "print(endseconds_since_epoch)\n",
    "\n",
    "interval = 10800 # 3 hr interval in seconds \n",
    "print(\"start loading in files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/parcels/lib/python3.9/genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/esd_data/Hycom_Wake/HYCOM_Wake_721688400.nc'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m expected_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/esd_data/Hycom_Wake/HYCOM_Wake_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seconds \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(startseconds_since_epoch, endseconds_since_epoch, interval)]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print(expected_files[0:5])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Filter the list to just the files that actually exist. (This is for when you remove NA files)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m actual_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m expected_files \u001b[38;5;28;01mif\u001b[39;00m isfile(f)]\n\u001b[1;32m      8\u001b[0m ds1 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_mfdataset(actual_files)  \u001b[38;5;66;03m# this puts the opendap data into a xarray dataset\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(ds1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [14], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m expected_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/esd_data/Hycom_Wake/HYCOM_Wake_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seconds \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(startseconds_since_epoch, endseconds_since_epoch, interval)]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print(expected_files[0:5])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Filter the list to just the files that actually exist. (This is for when you remove NA files)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m actual_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m expected_files \u001b[38;5;28;01mif\u001b[39;00m \u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m      8\u001b[0m ds1 \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_mfdataset(actual_files)  \u001b[38;5;66;03m# this puts the opendap data into a xarray dataset\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(ds1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/parcels/lib/python3.9/genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#file = glob.glob(\"/home/esd_data/Hycom_Wake/HYCOM_Wake_*.nc\")\n",
    "# create list of file paths between the start and end time in seconds sincce epoch \n",
    "expected_files = [f\"/home/esd_data/Hycom_Wake/HYCOM_Wake_{seconds}.nc\" for seconds in range(startseconds_since_epoch, endseconds_since_epoch, interval)]\n",
    "#print(expected_files[0:5])\n",
    "# Filter the list to just the files that actually exist. (This is for when you remove NA files)\n",
    "actual_files = [f for f in expected_files if isfile(f)]\n",
    "\n",
    "ds1 = xr.open_mfdataset(actual_files)  # this puts the opendap data into a xarray dataset\n",
    "#print(ds1)\n",
    "myDat1 = ds1.sel(**{'TIME': slice(startDate,endDate)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "variables = {'U': 'WATER_U',\n",
    "             'V': 'WATER_V'}\n",
    "dimensions = {'lon': 'LONGITUDE4001_4563',\n",
    "              'lat': 'LATITUDE1064_1376',\n",
    "              'time': 'TIME',\n",
    "              'depth': 'LEV1_20'}\n",
    "\n",
    "fieldset = FieldSet.from_xarray_dataset(myDat1, variables, dimensions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kh = 10.0   # This is the eddy diffusivity in m2/s\n",
    "fieldset.add_constant_field('Kh_zonal', kh, mesh='spherical')\n",
    "#zonal follows lat\n",
    "fieldset.add_constant_field('Kh_meridional', kh, mesh='spherical') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DisplacementParticle(JITParticle):\n",
    "    #dU = Variable('dU', to_write = False)\n",
    "    #dV = Variable('dV', to_write = False)\n",
    "    #d2s = Variable('d2s', initial=1e3, to_write = False)\n",
    "    age = Variable('age', dtype=np.float32, initial= 0., to_write = False)\n",
    "    cycle_phase=Variable('cycle_phase', dtype=np.float32, initial=0., to_write = False)\n",
    "    releaseSite = Variable('releaseSite', dtype=np.int32, to_write = False)\n",
    "    #distance = Variable('distance', dtype=np.int32, initial=0.) # not calculating distance for now but left this in\n",
    "    #prev_lat = Variable('prev_lat', initial=0., to_write=False)  \n",
    "    #prev_lon = Variable('prev_lon', initial=0., to_write=False)\n",
    "    f = Variable('f', dtype=np.int32, to_write = False)\n",
    "    driftlayer = Variable('driftlayer', dtype=np.int32, to_write = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "source_loc = pd.read_csv('/home/gmukai/Desktop/Wake_Parcel_Files/Wakedeeper_Taongi_Bikar_releasesites.csv', header=None)\n",
    "# Number of particle released per location\n",
    "npart_perlayer = 1\n",
    "npart = 10*npart_perlayer\n",
    "\n",
    "# Release location from the file read in above\n",
    "lon = np.repeat(source_loc[0], npart)\n",
    "lat = np.repeat(source_loc[1],npart)\n",
    "site = np.repeat(source_loc[2],npart)\n",
    "# Start date for release. Since we are releasing every set number of days the repeatdt version was simplest\n",
    "#start_date = 0\n",
    "dlayer = [0.25]*(len(source_loc)*npart)\n",
    "driftlayers = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # Define the layers\n",
    "repeating_layers = driftlayers * (len(source_loc)*npart_perlayer)\n",
    "\n",
    "repeatdt = timedelta(days=1)\n",
    "\n",
    "print(\"called in release points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"create pset now\")\n",
    "pset = ParticleSet.from_list(fieldset=fieldset, pclass=DisplacementParticle,lon=lon,lat=lat,releaseSite=site,\n",
    "                             depth=dlayer, repeatdt=repeatdt, driftlayer = repeating_layers)\n",
    "print(\"create kernels\")\n",
    "\n",
    "kernels = [EggHatchingMovement, displace, AdvectionRK4, DiffusionUniformKh, set_displacement, AgeDelete, DeleteErrorParticle]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output_file = pset.ParticleFile(name=\"Wake_1992_n10_n1perlayer_kh10_Oct_Feb_bounce_Bumphead6_17.zarr\", outputdt=timedelta(hours=0.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt=timedelta(minutes=10)\n",
    "\n",
    "run_days = 76\n",
    "print(\"start execute\")\n",
    "pset.execute(kernels,\n",
    "            runtime=timedelta(days=run_days),\n",
    "            dt=model_dt, \n",
    "            output_file=output_file)\n",
    "pset.repeatdt = None\n",
    "\n",
    "pset.execute(kernels,\n",
    "            runtime=timedelta(days=31+1),\n",
    "            dt=model_dt, \n",
    "            output_file=output_file)\n",
    "\n",
    "#data_xarray = xr.open_zarr(\"Wake_1992_n10_kh10_Oct_Feb_bounce5_2_2.zarr\")\n",
    "#data_xarray.to_netcdf(\"Wake_1992_n10_kh10_Oct_Feb_bounce5_2_2.nc\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parcels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
